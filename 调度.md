The scheduler's job is to distribute ready-to-run goroutines over worker threads

调度程序的任务是在工作线程上分发准备运行的Goroutine

G- goroutine
M-工作线程或者机器M
P-处理器，processor,执行go代码的需要的资源。

M 必须绑定P来执行go代码，

M 是一个os 线程。

所有的p都在一个数组中，这是工作调取的要求

https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw/edit#

http://supertech.csail.mit.edu/papers/steal.pdf

空闲p的无锁链表


<!-- Worker thread parking/unparking.
We need to balance between keeping enough running worker threads to utilize
available hardware parallelism and parking excessive running worker threads
to conserve CPU resources and power. This is not simple for two reasons:
(1) scheduler state is intentionally distributed (in particular, per-P work
queues), so it is not possible to compute global predicates on fast paths;
(2) for optimal thread management we would need to know the future (don't park
a worker thread when a new goroutine will be readied in near future).

Three rejected approaches that would work badly:
1. Centralize all scheduler state (would inhibit scalability).
2. Direct goroutine handoff. That is, when we ready a new goroutine and there
   is a spare P, unpark a thread and handoff it the thread and the goroutine.
   This would lead to thread state thrashing, as the thread that readied the
   goroutine can be out of work the very next moment, we will need to park it.
   Also, it would destroy locality of computation as we want to preserve
   dependent goroutines on the same thread; and introduce additional latency.
3. Unpark an additional thread whenever we ready a goroutine and there is an
   idle P, but don't do handoff. This would lead to excessive thread parking/
   unparking as the additional threads will instantly park without discovering
   any work to do.

The current approach:
We unpark an additional thread when we ready a goroutine if (1) there is an
idle P and there are no "spinning" worker threads. A worker thread is considered
spinning if it is out of local work and did not find work in global run queue/
netpoller; the spinning state is denoted in m.spinning and in sched.nmspinning.
Threads unparked this way are also considered spinning; we don't do goroutine
handoff so such threads are out of work initially. Spinning threads do some
spinning looking for work in per-P run queues before parking. If a spinning
thread finds work it takes itself out of the spinning state and proceeds to
execution. If it does not find work it takes itself out of the spinning state
and then parks.
If there is at least one spinning thread (sched.nmspinning>1), we don't unpark
new threads when readying goroutines. To compensate for that, if the last spinning
thread finds work and stops spinning, it must unpark a new spinning thread.
This approach smooths out unjustified spikes of thread unparking,
but at the same time guarantees eventual maximal CPU parallelism utilization.

The main implementation complication is that we need to be very careful during
spinning->non-spinning thread transition. This transition can race with submission
of a new goroutine, and either one part or another needs to unpark another worker
thread. If they both fail to do that, we can end up with semi-persistent CPU
underutilization. The general pattern for goroutine readying is: submit a goroutine
to local work queue, #StoreLoad-style memory barrier, check sched.nmspinning.
The general pattern for spinning->non-spinning transition is: decrement nmspinning,
#StoreLoad-style memory barrier, check all per-P work queues for new work.
Note that all this complexity does not apply to global run queue as we are not
sloppy about thread unparking when submitting to global queue. Also see comments
for nmspinning manipulation. -->

工人线程停放/取消停放。
我们需要在保持足够的运行工作线程以供利用之间取得平衡。
可用硬件并行度和停放过多正在运行的工作线程。
以节省CPU资源和电源。这并不简单，原因有二：
(1)故意分布调度器状态(特别是每P个工作。
队列)，因此不可能在快速路径上计算全局谓词；
(2)为了优化线程管理，我们需要知道未来(不要驻留。
在不久的将来准备好新的Goroutine时的工作线程)。

三种被拒绝、但效果不佳的方法：
1.集中所有调度器状态(会抑制可伸缩性)。
2.直接Goroutine切换。也就是说，当我们准备好一个新的大猩猩。
是一个空闲的P，取消驻留线程并将线程和Goroutine传递给它。
这将导致线程状态颠簸，因为准备好。
大猩猩可能在下一刻失业，我们需要把它停下来。
此外，它还会破坏我们想要保留的计算局部性。
依赖于同一线程上的Goroutines；并且引入了额外的延迟。
3.每当我们准备一个Goroutine并且有一个。
空闲P，但不做切换。这将导致线程过度驻留/。
取消停车，因为额外的线程会立即停车而不会发现。
任何要做的工作。

当前的方法是：
当我们准备一个Goroutine时，如果(1)有一个。
空闲P，并且没有“旋转”的工作线程。工作线程被认为是。
如果它不在本地工作且在全局运行队列中找不到工作，则旋转/。
Netpoller；旋转状态用m.sping表示，用Schedul.nmsping表示。
以这种方式停放的线程也被认为是旋转的；我们不执行goroutine。
切换，这样这样的线程最初就无法工作。旋转线做了一些。
在停车前旋转，在每个P运行队列中寻找工作。如果旋转。
线程找到工作后，它会使自己脱离旋转状态，并继续执行。
行刑。如果它找不到工作，它就会使自己脱离旋转状态。
然后去公园。
如果至少有一个旋转线程(Schedul.nmSpin>1)，我们就不会取消驻留。
准备Goroutine时的新线程。为了弥补这一点，如果最后一次旋转。
线程找到工作并停止旋转时，必须释放新的旋转线程。
这种方法消除了线程取消驻留的不合理尖峰，
但同时保证最终最大的CPU并行度利用率。

实现的主要复杂之处在于，在执行过程中需要非常小心。
旋转->非旋转螺纹过渡。这一转变可能会与屈服赛跑。
一辆新的大猩猩，其中一部分或另一部分需要让另一名工人离开。
线。如果它们都不能做到这一点，我们最终可能会得到半持久化的CPU。
利用不足。大猩猩准备的一般模式是：提交一份大猩猩。
对于本地工作队列#StoreLoad-Style内存屏障，请检查Schedul.nmsping。
旋转->非旋转转换的一般模式是：递减nmSpin，
#StoreLoad-Style内存屏障，检查所有Per-P工作队列是否有新工作。
请注意，所有这些复杂性都不适用于全局运行队列，因为我们不适用于全局运行队列。
提交到全局队列时对线程取消停放的草率。另请参阅评论。
用于nmSpin操作。

```golang
type g struct {
	// Stack parameters.
	// stack describes the actual stack memory: [stack.lo, stack.hi).
	// stackguard0 is the stack pointer compared in the Go stack growth prologue.
	// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.
	// stackguard1 is the stack pointer compared in the C stack growth prologue.
	// It is stack.lo+StackGuard on g0 and gsignal stacks.
	// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).
	stack       stack   // offset known to runtime/cgo
	stackguard0 uintptr // offset known to liblink
	stackguard1 uintptr // offset known to liblink

	_panic       *_panic // innermost panic - offset known to liblink
	_defer       *_defer // innermost defer
	m            *m      // current m; offset known to arm liblink
	sched        gobuf
	syscallsp    uintptr        // if status==Gsyscall, syscallsp = sched.sp to use during gc
	syscallpc    uintptr        // if status==Gsyscall, syscallpc = sched.pc to use during gc
	stktopsp     uintptr        // expected sp at top of stack, to check in traceback
	param        unsafe.Pointer // passed parameter on wakeup
	atomicstatus uint32
	stackLock    uint32 // sigprof/scang lock; TODO: fold in to atomicstatus
	goid         int64
	schedlink    guintptr
	waitsince    int64      // approx time when the g become blocked
	waitreason   waitReason // if status==Gwaiting

	preempt       bool // preemption signal, duplicates stackguard0 = stackpreempt
	preemptStop   bool // transition to _Gpreempted on preemption; otherwise, just deschedule
	preemptShrink bool // shrink stack at synchronous safe point

	// asyncSafePoint is set if g is stopped at an asynchronous
	// safe point. This means there are frames on the stack
	// without precise pointer information.
	asyncSafePoint bool

	paniconfault bool // panic (instead of crash) on unexpected fault address
	gcscandone   bool // g has scanned stack; protected by _Gscan bit in status
	throwsplit   bool // must not split stack
	// activeStackChans indicates that there are unlocked channels
	// pointing into this goroutine's stack. If true, stack
	// copying needs to acquire channel locks to protect these
	// areas of the stack.
	activeStackChans bool
	// parkingOnChan indicates that the goroutine is about to
	// park on a chansend or chanrecv. Used to signal an unsafe point
	// for stack shrinking. It's a boolean value, but is updated atomically.
	parkingOnChan uint8

	raceignore     int8     // ignore race detection events
	sysblocktraced bool     // StartTrace has emitted EvGoInSyscall about this goroutine
	sysexitticks   int64    // cputicks when syscall has returned (for tracing)
	traceseq       uint64   // trace event sequencer
	tracelastp     puintptr // last P emitted an event for this goroutine
	lockedm        muintptr
	sig            uint32
	writebuf       []byte
	sigcode0       uintptr
	sigcode1       uintptr
	sigpc          uintptr
	gopc           uintptr         // pc of go statement that created this goroutine
	ancestors      *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors)
	startpc        uintptr         // pc of goroutine function
	racectx        uintptr
	waiting        *sudog         // sudog structures this g is waiting on (that have a valid elem ptr); in lock order
	cgoCtxt        []uintptr      // cgo traceback context
	labels         unsafe.Pointer // profiler labels
	timer          *timer         // cached timer for time.Sleep
	selectDone     uint32         // are we participating in a select and did someone win the race?

	// Per-G GC state

	// gcAssistBytes is this G's GC assist credit in terms of
	// bytes allocated. If this is positive, then the G has credit
	// to allocate gcAssistBytes bytes without assisting. If this
	// is negative, then the G must correct this by performing
	// scan work. We track this in bytes to make it fast to update
	// and check for debt in the malloc hot path. The assist ratio
	// determines how this corresponds to scan work debt.
	gcAssistBytes int64
}
```

M 结构体

```golang
type m struct {
	g0      *g     // goroutine with scheduling stack
	morebuf gobuf  // gobuf arg to morestack
	divmod  uint32 // div/mod denominator for arm - known to liblink

	// Fields not known to debuggers.
	procid        uint64       // for debuggers, but offset not hard-coded
	gsignal       *g           // signal-handling g
	goSigStack    gsignalStack // Go-allocated signal handling stack
	sigmask       sigset       // storage for saved signal mask
	tls           [6]uintptr   // thread-local storage (for x86 extern register)
	mstartfn      func()
	curg          *g       // current running goroutine
	caughtsig     guintptr // goroutine running during fatal signal
	p             puintptr // attached p for executing go code (nil if not executing go code)
	nextp         puintptr
	oldp          puintptr // the p that was attached before executing a syscall
	id            int64
	mallocing     int32
	throwing      int32
	preemptoff    string // if != "", keep curg running on this m
	locks         int32
	dying         int32
	profilehz     int32
	spinning      bool // m is out of work and is actively looking for work
	blocked       bool // m is blocked on a note
	newSigstack   bool // minit on C thread called sigaltstack
	printlock     int8
	incgo         bool   // m is executing a cgo call
	freeWait      uint32 // if == 0, safe to free g0 and delete m (atomic)
	fastrand      [2]uint32
	needextram    bool
	traceback     uint8
	ncgocall      uint64      // number of cgo calls in total
	ncgo          int32       // number of cgo calls currently in progress
	cgoCallersUse uint32      // if non-zero, cgoCallers in use temporarily
	cgoCallers    *cgoCallers // cgo traceback if crashing in cgo call
	doesPark      bool        // non-P running threads: sysmon and newmHandoff never use .park
	park          note
	alllink       *m // on allm
	schedlink     muintptr
	lockedg       guintptr
	createstack   [32]uintptr // stack that created this thread.
	lockedExt     uint32      // tracking for external LockOSThread
	lockedInt     uint32      // tracking for internal lockOSThread
	nextwaitm     muintptr    // next m waiting for lock
	waitunlockf   func(*g, unsafe.Pointer) bool
	waitlock      unsafe.Pointer
	waittraceev   byte
	waittraceskip int
	startingtrace bool
	syscalltick   uint32
	freelink      *m // on sched.freem

	// mFixup is used to synchronize OS related m state (credentials etc)
	// use mutex to access.
	mFixup struct {
		lock mutex
		fn   func(bool) bool
	}

	// these are here because they are too large to be on the stack
	// of low-level NOSPLIT functions.
	libcall   libcall
	libcallpc uintptr // for cpu profiler
	libcallsp uintptr
	libcallg  guintptr
	syscall   libcall // stores syscall parameters on windows

	vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call)
	vdsoPC uintptr // PC for traceback while in VDSO call

	// preemptGen counts the number of completed preemption
	// signals. This is used to detect when a preemption is
	// requested, but fails. Accessed atomically.
	preemptGen uint32

	// Whether this is a pending preemption signal on this M.
	// Accessed atomically.
	signalPending uint32

	dlogPerM

	mOS

	// Up to 10 locks held by this m, maintained by the lock ranking code.
	locksHeldLen int
	locksHeld    [10]heldLockInfo
}
```




P 结构体

```golang
type p struct {
	id          int32
	status      uint32 // one of pidle/prunning/...
	link        puintptr
	schedtick   uint32     // incremented on every scheduler call
	syscalltick uint32     // incremented on every system call
	sysmontick  sysmontick // last tick observed by sysmon
	m           muintptr   // back-link to associated m (nil if idle)
	mcache      *mcache
	pcache      pageCache
	raceprocctx uintptr

	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
	deferpoolbuf [5][32]*_defer

	// Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.
	goidcache    uint64
	goidcacheend uint64

	// Queue of runnable goroutines. Accessed without lock.
	runqhead uint32
	runqtail uint32
	runq     [256]guintptr
	// runnext, if non-nil, is a runnable G that was ready'd by
	// the current G and should be run next instead of what's in
	// runq if there's time remaining in the running G's time
	// slice. It will inherit the time left in the current time
	// slice. If a set of goroutines is locked in a
	// communicate-and-wait pattern, this schedules that set as a
	// unit and eliminates the (potentially large) scheduling
	// latency that otherwise arises from adding the ready'd
	// goroutines to the end of the run queue.
	runnext guintptr

	// Available G's (status == Gdead)
	gFree struct {
		gList
		n int32
	}

	sudogcache []*sudog
	sudogbuf   [128]*sudog

	// Cache of mspan objects from the heap.
	mspancache struct {
		// We need an explicit length here because this field is used
		// in allocation codepaths where write barriers are not allowed,
		// and eliminating the write barrier/keeping it eliminated from
		// slice updates is tricky, moreso than just managing the length
		// ourselves.
		len int
		buf [128]*mspan
	}

	tracebuf traceBufPtr

	// traceSweep indicates the sweep events should be traced.
	// This is used to defer the sweep start event until a span
	// has actually been swept.
	traceSweep bool
	// traceSwept and traceReclaimed track the number of bytes
	// swept and reclaimed by sweeping in the current sweep loop.
	traceSwept, traceReclaimed uintptr

	palloc persistentAlloc // per-P to avoid mutex

	_ uint32 // Alignment for atomic fields below

	// The when field of the first entry on the timer heap.
	// This is updated using atomic functions.
	// This is 0 if the timer heap is empty.
	timer0When uint64

	// The earliest known nextwhen field of a timer with
	// timerModifiedEarlier status. Because the timer may have been
	// modified again, there need not be any timer with this value.
	// This is updated using atomic functions.
	// This is 0 if the value is unknown.
	timerModifiedEarliest uint64

	// Per-P GC state
	gcAssistTime         int64 // Nanoseconds in assistAlloc
	gcFractionalMarkTime int64 // Nanoseconds in fractional mark worker (atomic)

	// gcMarkWorkerMode is the mode for the next mark worker to run in.
	// That is, this is used to communicate with the worker goroutine
	// selected for immediate execution by
	// gcController.findRunnableGCWorker. When scheduling other goroutines,
	// this field must be set to gcMarkWorkerNotWorker.
	gcMarkWorkerMode gcMarkWorkerMode
	// gcMarkWorkerStartTime is the nanotime() at which the most recent
	// mark worker started.
	gcMarkWorkerStartTime int64

	// gcw is this P's GC work buffer cache. The work buffer is
	// filled by write barriers, drained by mutator assists, and
	// disposed on certain GC state transitions.
	gcw gcWork

	// wbBuf is this P's GC write barrier buffer.
	//
	// TODO: Consider caching this in the running G.
	wbBuf wbBuf

	runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point

	// statsSeq is a counter indicating whether this P is currently
	// writing any stats. Its value is even when not, odd when it is.
	statsSeq uint32

	// Lock for timers. We normally access the timers while running
	// on this P, but the scheduler can also do it from a different P.
	timersLock mutex

	// Actions to take at some time. This is used to implement the
	// standard library's time package.
	// Must hold timersLock to access.
	timers []*timer

	// Number of timers in P's heap.
	// Modified using atomic instructions.
	numTimers uint32

	// Number of timerModifiedEarlier timers on P's heap.
	// This should only be modified while holding timersLock,
	// or while the timer status is in a transient state
	// such as timerModifying.
	adjustTimers uint32

	// Number of timerDeleted timers in P's heap.
	// Modified using atomic instructions.
	deletedTimers uint32

	// Race context used while executing timer functions.
	timerRaceCtx uintptr

	// preempt is set to indicate that this P should be enter the
	// scheduler ASAP (regardless of what G is running on it).
	preempt bool

	pad cpu.CacheLinePad
}
```


当一个新的G被创建或现有的G变为可运行时，它被放入
当前P的可运行goroutine的列表。当P完成执行G时，
它首先尝试从可运行的G中弹出一个G。如果该列表为空，P将随机选择一个受害者（另一个P），
并试图从该列表中窃取一半可运行的G。

自旋与非自旋状态。

p 记录当前m，有一个可运行的goroutine队列。
g 记录当前m，最后执行的P
m 记录g0，当前执行的g，当前的p，当前空闲的p

- 全局队列需要加锁，因为是多个线程共享的